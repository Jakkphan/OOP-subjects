{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Classification and Sorting of Coffee beans Using Deep Learning.**"
      ],
      "metadata": {
        "id": "sv_9enzLyX9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DATA**"
      ],
      "metadata": {
        "id": "pZJLBIHFIFVg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read and access data from Google Drive"
      ],
      "metadata": {
        "id": "9xy9cm3DIPmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "sw2jZqRpIV57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the required packages"
      ],
      "metadata": {
        "id": "x5WFWU92IZsi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.utils.data as data\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "import time, os, copy, argparse\n",
        "import multiprocessing\n",
        "from torchsummary import summary\n",
        "from matplotlib import pyplot as plt\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "u9HVT7w4IqwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorboard"
      ],
      "metadata": {
        "id": "c-gDlTIRIu60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Launch the program code with the environment variable CUDA_LAUNCH_BLOCKING"
      ],
      "metadata": {
        "id": "Ti62SSoRI774"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "PCTlppEyI4uW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create train, validation and test from directory paths in Google Drive"
      ],
      "metadata": {
        "id": "GjsH7-CsJE64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create train, validate and test from the dataset path\n",
        "train_dir = '/content/drive/MyDrive/Coffee/dataset/train'\n",
        "\n",
        "val_dir = '/content/drive/MyDrive/Coffee/dataset/val'\n",
        "\n",
        "test_dir = '/content/drive/MyDrive/Coffee/dataset/test'"
      ],
      "metadata": {
        "id": "O18urxFGJOhJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create a starting variable for the experiment"
      ],
      "metadata": {
        "id": "Orl6kJTXK304"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create variables for multiple epochs\n",
        "num_epochs = 25\n",
        "\n",
        "# Create variables for batch size\n",
        "batch_size = 4\n",
        "\n",
        "# variable for the number of classes\n",
        "num_classes = 4\n",
        "\n",
        "# variable for num_workers which tells how many processor used to decomposethe data\n",
        "num_cpu = multiprocessing.cpu_count()"
      ],
      "metadata": {
        "id": "hnNQNPubPSdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calculate mean and standard deviation (STD) values for image normalization"
      ],
      "metadata": {
        "id": "HXh0OlW9LnmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load and display an example of the image of the coffee bean that will be calculate\n",
        "\n",
        "# Loading images\n",
        "path_base ='/content/drive/MyDrive/Coffee/dataset/train'\n",
        "img_path = path_base + '/longberry/1010.jpg'\n",
        "img = Image.open(img_path)\n",
        "img"
      ],
      "metadata": {
        "id": "SYp58EwzLwhr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert PIL image to numpy array\n",
        "img_np = np.array(img)\n",
        "\n",
        "\n",
        "# Plots the value of each pixel of the image\n",
        "plt.hist(img_np.ravel(), bins=50, density=True)\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"frequency\")\n",
        "plt.title(\"Distribution of each pixel\")"
      ],
      "metadata": {
        "id": "m1Z-Tru6Sq9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image tranforms to PyTorch tensor\n",
        "transform = transforms. Compose([\n",
        "    transforms. ToTensor()\n",
        "])\n",
        "\n",
        "# Transform from PIL image to tensor\n",
        "img_tr = transform(img)\n",
        "\n",
        "# Convert tensor images to numpy arrays\n",
        "img_np = np.array(img_tr)\n",
        "\n",
        "# Plots tha value of each pixel of the image\n",
        "plt.hist(img_np.ravel(), bins=50, density=True)\n",
        "plt.xlabel(\"Pixel Value\")\n",
        "plt.ylabel(\"frequency\")\n",
        "plt.title(\"Distribution of each pixel\")"
      ],
      "metadata": {
        "id": "41plP1Q9S2a_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate mean and standard deviasi (STD) values\n",
        "\n",
        "# Tensor image\n",
        "img_tr = transform(img)\n",
        "\n",
        "# Calculating mean and std\n",
        "mean, std = img_tr.mean([1,2]), img_tr.std([1,2])\n",
        "\n",
        "# Print the calculation results\n",
        "print(\"Average value (mean) and standard deviation (STD) before normalization:\")\n",
        "print(\"Average (mean) of the image:\", mean)\n",
        "print(\"Standard deviation (std) of the image:\", std)"
      ],
      "metadata": {
        "id": "2esKFAkiacyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the value before normalization with thenormalized value\n",
        "\n",
        "# Create image tranforms to PyTorch tensor and normalization of mean and std values\n",
        "transform_norm = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std)\n",
        "])\n",
        "\n",
        "# Of images to be normalized\n",
        "img_normalized = transform_norm(img)\n",
        "\n",
        "# Convert normalized image to numpy arrays\n",
        "img_np = np.array(img_normalized)\n",
        "\n",
        "# Plots the value of each pixel of the image\n",
        "plt.hist(img_np.ravel(), bins=50, density=True)\n",
        "plt.xlabel(\"Pixel value\")\n",
        "plt.ylabel(\"frequency\")\n",
        "plt.title(\"Distribution of each pixel\")"
      ],
      "metadata": {
        "id": "9LtvYoGYauvt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TUpOEVqyVLe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying normalized image results\n",
        "\n",
        "img_normalized = transform_norm(img)\n",
        "\n",
        "# Convert images to numpy arrays\n",
        "img_normalized = np.array(img_normalized)\n",
        "\n",
        "# Transpose from shape(3,,) to shape(,,3)\n",
        "img_normalized = img_normalized.transpose(1, 2, 0)\n",
        "\n",
        "# Displaying the results of normalized images\n",
        "plt.imshow(img_normalized)\n",
        "plt.xticks([])\n",
        "plt.yticks([])"
      ],
      "metadata": {
        "id": "qUcRbSXta2-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the mean and std values after normalization where the mean value is 0.0 and the std value of normalized images\n",
        "\n",
        "img_nor = transform_norm(img)\n",
        "\n",
        "# Calculate the average value and standard deviation\n",
        "mean, std = img_nor.mean([1,2]), img_nor.std([1,2])\n",
        "\n",
        "# Prints the results of the average score and standard of the deviasi\n",
        "print(\"Average values and standard deviations from the normalization of the image:\")\n",
        "print(\"Average value of the image:\", mean)\n",
        "print(\"Standard value deviation from the image:\", std)"
      ],
      "metadata": {
        "id": "P_bKMxqIbAvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating Data Transform"
      ],
      "metadata": {
        "id": "WgtR8DaBWaRd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the mean value variables (mean) and standard deviation (STD) for use in the normalization of the image\n",
        "mean = [0.7980, 0.7820, 0.7562]\n",
        "std = [0.1441, 0.1729, 0.2341]"
      ],
      "metadata": {
        "id": "xiCJprNHcAZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform image data to match when training\n",
        "image_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "                                 transforms.Resize(size=(256, 256)),\n",
        "                                 transforms.RandomHorizontalFlip(),\n",
        "                                 transforms.ToTensor(),\n",
        "                                 transforms.Normalize(mean, std)\n",
        "                                 ]),\n",
        "    'val': transforms.Compose([\n",
        "                               transforms.Resize(size=(256, 256)),\n",
        "                               transforms.ToTensor(),\n",
        "                               transforms.Normalize(mean, std)\n",
        "                               ]),\n",
        "    'test': transforms.Compose([\n",
        "                                transforms.Resize(size=(256, 256)),\n",
        "                                transforms.ToTensor(),\n",
        "                                transforms.Normalize(mean, std)\n",
        "                                ]),\n",
        "                    }"
      ],
      "metadata": {
        "id": "GjWyEKs5QuL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Summary and Visualization"
      ],
      "metadata": {
        "id": "CzyQrSw7WzxR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data from a folder\n",
        "dataset = {\n",
        "    'train': datasets.ImageFolder(root=train_dir, transform=image_transforms['train']),\n",
        "    'val': datasets.ImageFolder(root=val_dir, transform=image_transforms['val']),\n",
        "    'test': datasets.ImageFolder(root=test_dir, transform=image_transforms['test']),\n",
        "}"
      ],
      "metadata": {
        "id": "CxcShRLuREGb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YjLe8JKAW6vQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Multiple classes on the dataset\n",
        "classes = dataset['train'].classes\n",
        "print('Many classes in this dataset are', len(classes),'and classes in the dataset consist of:', classes)"
      ],
      "metadata": {
        "id": "aZ0TR5CO2zOZ",
        "outputId": "0775457e-6634-439f-c246-bfcb7d324eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-921f0594e787>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Multiple classes on the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Many classes in this dataset are'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'and classes in the dataset consist of:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0iKzNOaWXH6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate a lot of data from training data, validation data and test data\n",
        "dataset_sizes = {\n",
        "    'train':len(dataset['train']),\n",
        "    'val':len(dataset['val']),\n",
        "    'test':len(dataset['test'])\n",
        "}"
      ],
      "metadata": {
        "id": "b8tzoH3mRNE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qO_vPK_eXag6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a large number of datasets\n",
        "print(\"Image data in the train data:\",dataset_sizes['train'])\n",
        "print(\"Image data in the valid data:\", dataset_sizes['val'])\n",
        "print(\"Image data in the test data:\", dataset_sizes['test'])"
      ],
      "metadata": {
        "id": "Vz-wgpMeRvtE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AW2X6JRyYCr8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an iterator to load the dataset\n",
        "dataloaders = {\n",
        "    'train' : data.DataLoader(dataset['train'], batch_size=batch_size, shuffle=True, num_workers=num_cpu, pin_memory=True, drop_last=True),\n",
        "    'val'   : data.DataLoader(dataset['val'], batch_size=batch_size, shuffle=True, num_workers=num_cpu, pin_memory=True, drop_last=True),\n",
        "    'test'  : data.DataLoader(dataset['test'], batch_size=batch_size, shuffle=True, num_workers=num_cpu, pin_memory=True, drop_last=True)\n",
        "}\n",
        "\n",
        "print('a lot of the amount of training data per class is', len(dataloaders['train']), 'image')\n",
        "print('a lot of validation data per class is', len(dataloaders['val']), 'image')\n",
        "print('a lot of the amount of test data per class is', len(dataloaders['test']), 'image')"
      ],
      "metadata": {
        "id": "yiG3GzpfRTA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of batch sizes, color outputs and the number of image in pixels\n",
        "\n",
        "images, labels = next(iter(dataloaders['train']))\n",
        "print(\"jumlah kelas adalah\", len(labels))\n",
        "print(\"dimensi ukuran gambar adalah\", images.shape)"
      ],
      "metadata": {
        "id": "P6RuiStL6J1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# visualization of image data on datasets\n",
        "\n",
        "def imshow(inp, title=None):\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = [0.7980, 0.7820, 0.7562]\n",
        "    std = [0.1441, 0.1729, 0.2341]\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001) # pause to update the plot of the image\n",
        "\n",
        "# Retrieving image data from training data\n",
        "inputs, classes = next(iter(dataloaders['train']))\n",
        "class_names = dataset['train'].classes\n",
        "# Create a grid\n",
        "out = torchvision.utils.make_grid(inputs)\n",
        "\n",
        "imshow(out, title=[class_names[x] for x in classes])"
      ],
      "metadata": {
        "id": "CV1zHcX7R9YO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1"
      ],
      "metadata": {
        "id": "8TYdgHhHir3N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device default"
      ],
      "metadata": {
        "id": "B1TzaFSHi0-8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the default device as GPU, if any\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "oIt3mH68RzIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = images.to(device), labels.to(device)"
      ],
      "metadata": {
        "id": "L56Wz0L5jGjV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment1:Resnet18"
      ],
      "metadata": {
        "id": "t1Upy2ezjLEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment 1: Using a pretrained model with the Resnet18 architecture\n",
        "\n",
        "print(\"\\Load Resnet18\\n\")\n",
        "model_ft = models.resnet18(pretrained=True)\n",
        "\n",
        "num_ftrs = model_ft.fc.in_features\n",
        "model_ft.fc = nn.Linear(num_ftrs,num_classes )\n",
        "\n",
        "# Transfer model to GPU\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "for num, (name, param) in enumerate(model_ft.named_parameters()):\n",
        "    print(num, name, param.requires_grad )\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Learning rate\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "3U-YdRx0d1GD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print model summary\n",
        "print('Model Summary:-\\n')\n",
        "summary(model_ft, input_size=(3, 256, 256))\n",
        "print(model_ft)"
      ],
      "metadata": {
        "id": "Mpi8ihNIU9g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer=SummaryWriter('/content/content/runs/summary_1')\n",
        "writer.add_graph(model_ft, images)"
      ],
      "metadata": {
        "id": "n4tnM14OXahq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print(\"\\nTraining:-\\n\")\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Tensorboard summary\n",
        "    writer = SummaryWriter(\"content/runs/model_percobaan_1\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Record training loss and accuracy for each phase\n",
        "            if phase == 'train':\n",
        "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
        "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
        "                writer.flush()\n",
        "            else:\n",
        "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
        "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
        "                writer.flush()\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "    writer.close()\n",
        "\n",
        "    %load_ext tensorboard\n",
        "    %tensorboard --logdir content"
      ],
      "metadata": {
        "id": "wLn_6YUAmsTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                       num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "cxdReaiJoF91"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir content/runs\n",
        "%reload_ext tensorboard"
      ],
      "metadata": {
        "id": "I6-9msUkLxK5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorboard import notebook\n",
        "notebook.list() # View open TensorBoard instances"
      ],
      "metadata": {
        "id": "stzY_4FMHq3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "ScTvzjmKkXO3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "PATH = 'model_percobaan_1.pth'\n",
        "print(\"\\nSaving the model...\")\n",
        "torch.save(model_ft, PATH)"
      ],
      "metadata": {
        "id": "8Tfh8NMakbIQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predicting from validation data"
      ],
      "metadata": {
        "id": "lUyWsIXLkmZx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model predictions\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "        visualize_model(model_ft)"
      ],
      "metadata": {
        "id": "kQqvohMzoSrh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict from test data"
      ],
      "metadata": {
        "id": "5Js3snRfk8vx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model predictions\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "visualize_model(model_ft)"
      ],
      "metadata": {
        "id": "YtUKxPYnNuSJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load accuracy from test data"
      ],
      "metadata": {
        "id": "5TmLNkhklJW_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTestSetAccuracy(model, criterion):\n",
        "    '''\n",
        "    Function to compute the accuracy on the test set\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate((dataloaders['test'])):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            test_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    # Find average test loss and test accuracy\n",
        "    avg_test_loss = test_loss/(dataset_sizes['test'])\n",
        "    avg_test_acc = test_acc/(dataset_sizes['test'])\n",
        "\n",
        "    print(\"\\nTest accuracy : \" + str(avg_test_acc))"
      ],
      "metadata": {
        "id": "YsRdQZBWRQd7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "computeTestSetAccuracy(model_ft, criterion)"
      ],
      "metadata": {
        "id": "bZdVxdjnS57s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform detection tests on test data"
      ],
      "metadata": {
        "id": "qSjpEduPl3T-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a mapping of the indices to the class names, in order to see the output classes of the test images.\n",
        "idx_to_class = {v: k for k, v in dataset['train'].class_to_idx.items()}\n",
        "print(idx_to_class)"
      ],
      "metadata": {
        "id": "Q9ix4hZa1W_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, test_image_name):\n",
        "    '''\n",
        "    Function to predict the class of a single test image\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param test_image_name: Test image\n",
        "    '''\n",
        "\n",
        "    transform = image_transforms['test']\n",
        "\n",
        "\n",
        "    test_image = Image.open(test_image_name)\n",
        "    plt.imshow(test_image)\n",
        "\n",
        "    test_image_tensor = transform(test_image)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 256, 256).cuda()\n",
        "    else:\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 256, 256)\n",
        "\n",
        "#    idx_to_class = {\n",
        "#      \"1\":\"defect\",\n",
        "#      \"2\":\"longberry\",\n",
        "#      \"3\":\"peaberry\",\n",
        "#      \"4\":\"premium\",\n",
        "#    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = torch.exp(out)\n",
        "\n",
        "        topk, topclass = ps.topk(3, dim=1)\n",
        "        cls = idx_to_class[topclass.cpu().numpy()[0][0]]\n",
        "        score = topk.cpu().numpy()[0][0]\n",
        "\n",
        "        for i in range(3):\n",
        "            print(\"Prediction\", i+1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]], \", Score: \", topk.cpu().numpy()[0][i])"
      ],
      "metadata": {
        "id": "6eXfL4CxO6m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def predict(model, test_image_name):\n",
        "    '''\n",
        "    Function to predict the class of a single test image\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param test_image_name: Test image\n",
        "    '''\n",
        "\n",
        "    transform = image_transforms['test']\n",
        "\n",
        "\n",
        "    test_image = Image.open(test_image_name)\n",
        "    plt.imshow(test_image)\n",
        "\n",
        "    test_image_tensor = transform(test_image)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 256, 256).cuda()\n",
        "    else:\n",
        "        test_image_tensor = test_image_tensor.view(1, 3, 256, 256)\n",
        "\n",
        "#    idx_to_class = {\n",
        "#      \"1\":\"defect\",\n",
        "#      \"2\":\"longberry\",\n",
        "#      \"3\":\"peaberry\",\n",
        "#      \"4\":\"premium\",\n",
        "#    }\n",
        "\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        # Model outputs log probabilities\n",
        "        out = model(test_image_tensor)\n",
        "        ps = F.softmax(out.data,dim=1)\n",
        "\n",
        "        topk, topclass = ps.topk(4, dim=1)\n",
        "        cls = idx_to_class[topclass.cpu().numpy()[0][0]]\n",
        "        score = topk.cpu().numpy()[0][0]\n",
        "\n",
        "        for i in range(4):\n",
        "            print(\"Prediction\", i+1, \":\", idx_to_class[topclass.cpu().numpy()[0][i]], \",\", end=\" \")\n",
        "            print(\"Score: {:.4f}%\".format(topk.cpu().numpy()[0][i] * 100))"
      ],
      "metadata": {
        "id": "yduYAA6AHKVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluating the model"
      ],
      "metadata": {
        "id": "O-De9CHOv42Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = 4\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(dataloaders['test']):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model_ft(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "Xua2574a19ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_confusion_matrix(cm,\n",
        "                          target_names,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=None,\n",
        "                          normalize=True):\n",
        "\n",
        "    import matplotlib.pyplot as plt\n",
        "    import numpy as np\n",
        "    import itertools\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "\n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "spQ9-pd-Wlks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(cm           = np.array([[349.,  15.,  11.,  25.],\n",
        "        [  3., 364.,   4.,  29.],\n",
        "        [  6.,   0., 285., 109.],\n",
        "        [ 20.,   5.,   4., 371.]]),\n",
        "                      normalize    = True,\n",
        "                      target_names = ['defect', 'longberry', 'peaberry', 'premium'],\n",
        "                      title        = \"Confusion Matrix\")"
      ],
      "metadata": {
        "id": "ZaXzreWoal1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To get the per-class accuracy:\n",
        "\n",
        "print(confusion_matrix.diag()/confusion_matrix.sum(1))"
      ],
      "metadata": {
        "id": "Fm2KHato2QfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXPERIMENT 2"
      ],
      "metadata": {
        "id": "KEUOapvAwTp1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device default"
      ],
      "metadata": {
        "id": "nmD4svSswYUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the default device as GPU, if any\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "OKu98dEuhjzh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = images.to(device), labels.to(device)"
      ],
      "metadata": {
        "id": "RHJkdGa0uDLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment 2: MobuleNetV2"
      ],
      "metadata": {
        "id": "Yvc6z2B7wigX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained model as MobileNetV2\n",
        "print(\"\\nLoading mobilenetv2 as feature extractor ...\\n\")\n",
        "model_tl = models.mobilenet_v2(pretrained=True)\n",
        "\n",
        "# Freeze all the required layers (i.e except last conv block and fc layers)\n",
        "for params in list(model_tl.parameters())[0:-5]:\n",
        "  params.requires_grad = False\n",
        "\n",
        "# Modify fc layers to match num_classes\n",
        "num_ftrs=model_tl.classifier[-1].in_features\n",
        "\n",
        "model_tl.classifier=nn.Sequential(\n",
        "    nn.Dropout(p=0.2, inplace=False),\n",
        "    nn.Linear(in_features=num_ftrs, out_features=num_classes, bias=True)\n",
        "    )\n",
        "\n",
        "# Transfer the model to GPU\n",
        "model_tl = model_tl.to(device)\n",
        "\n",
        "# Print model summary\n",
        "print('Model Summary:-\\n')\n",
        "for num, (name, param) in enumerate(model_tl.named_parameters()):\n",
        "    print(num, name, param.requires_grad )\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimizer\n",
        "optimizer_tl = optim.Adam(model_tl.parameters(), lr=0.001)\n",
        "\n",
        "# Learning rate decay\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_tl, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "ZCYXQdAnXCU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print model summary\n",
        "print('Model Summary:-\\n')\n",
        "summary(model_tl, input_size=(3, 256, 256))\n",
        "print(model_tl)"
      ],
      "metadata": {
        "id": "6DZ8B2Viy0GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "writer=SummaryWriter('/content/content/runs/summary_model_2')\n",
        "writer.add_graph(model_tl, images)"
      ],
      "metadata": {
        "id": "6xUjyZnIumZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model on experiment 2"
      ],
      "metadata": {
        "id": "jLjrxf1ew_ty"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "print(\"\\nTraining:-\\n\")\n",
        "\n",
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=30):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    # Tensorboard summary\n",
        "    writer = SummaryWriter(\"content/runs/model_percobaan_2\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device, non_blocking=True)\n",
        "                labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Record training loss and accuracy for each phase\n",
        "            if phase == 'train':\n",
        "                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n",
        "                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n",
        "                writer.flush()\n",
        "            else:\n",
        "                writer.add_scalar('Valid/Loss', epoch_loss, epoch)\n",
        "                writer.add_scalar('Valid/Accuracy', epoch_acc, epoch)\n",
        "                writer.flush()\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model\n",
        "\n",
        "    writer.close()"
      ],
      "metadata": {
        "id": "Amw66djjpPAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training model\n",
        "model_tl = train_model(model_tl, criterion, optimizer_tl, exp_lr_scheduler,\n",
        "                       num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "Q3MhLIG6olhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "PATH = 'model_percobaan_2.pth'\n",
        "print(\"\\nSaving the model...\")\n",
        "torch.save(model_tl, PATH)"
      ],
      "metadata": {
        "id": "MflTKbS0zRLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict from validation data"
      ],
      "metadata": {
        "id": "1E9Rw5uKxQMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model predictions\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)"
      ],
      "metadata": {
        "id": "0F5kkgQQ161U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model(model_tl)"
      ],
      "metadata": {
        "id": "24aa2DiH2Fdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Predict from test data"
      ],
      "metadata": {
        "id": "ncGu7vB8xW-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model predictions\n",
        "\n",
        "def visualize_model(model, num_images=6):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    images_so_far = 0\n",
        "    fig = plt.figure()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(dataloaders['test']):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            for j in range(inputs.size()[0]):\n",
        "                images_so_far += 1\n",
        "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
        "                ax.axis('off')\n",
        "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
        "                imshow(inputs.cpu().data[j])\n",
        "\n",
        "                if images_so_far == num_images:\n",
        "                    model.train(mode=was_training)\n",
        "                    return\n",
        "        model.train(mode=was_training)\n",
        "\n",
        "visualize_model(model_tl)"
      ],
      "metadata": {
        "id": "RHanBiEg2ZTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load accuracy from test data"
      ],
      "metadata": {
        "id": "LYq9Ou4exdJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def computeTestSetAccuracy(model, criterion):\n",
        "    '''\n",
        "    Function to compute the accuracy on the test set\n",
        "    Parameters\n",
        "        :param model: Model to test\n",
        "        :param loss_criterion: Loss Criterion to minimize\n",
        "    '''\n",
        "\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    test_acc = 0.0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    # Validation - No gradient tracking needed\n",
        "    with torch.no_grad():\n",
        "\n",
        "        # Set to evaluation mode\n",
        "        model.eval()\n",
        "\n",
        "        # Validation loop\n",
        "        for j, (inputs, labels) in enumerate((dataloaders['test'])):\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass - compute outputs on input data using the model\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Compute loss\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Compute the total loss for the batch and add it to valid_loss\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "            # Calculate validation accuracy\n",
        "            ret, predictions = torch.max(outputs.data, 1)\n",
        "            correct_counts = predictions.eq(labels.data.view_as(predictions))\n",
        "\n",
        "            # Convert correct_counts to float and then compute the mean\n",
        "            acc = torch.mean(correct_counts.type(torch.FloatTensor))\n",
        "\n",
        "            # Compute total accuracy in the whole batch and add to valid_acc\n",
        "            test_acc += acc.item() * inputs.size(0)\n",
        "\n",
        "            print(\"Test Batch number: {:03d}, Test: Loss: {:.4f}, Accuracy: {:.4f}\".format(j, loss.item(), acc.item()))\n",
        "\n",
        "    # Find average test loss and test accuracy\n",
        "    avg_test_loss = test_loss/(dataset_sizes['test'])\n",
        "    avg_test_acc = test_acc/(dataset_sizes['test'])\n",
        "\n",
        "    print(\"\\nTest accuracy : \" + str(avg_test_acc))"
      ],
      "metadata": {
        "id": "MY_XcvQh2-tp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "computeTestSetAccuracy(model_tl, criterion)"
      ],
      "metadata": {
        "id": "Kh3m22nm3JE8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perform detection tests on test data"
      ],
      "metadata": {
        "id": "ZHgbaVDJxox2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluatin the Model"
      ],
      "metadata": {
        "id": "iDsRvhQex3kM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nb_classes = 4\n",
        "\n",
        "confusion_matrix = torch.zeros(nb_classes, nb_classes)\n",
        "with torch.no_grad():\n",
        "    for i, (inputs, classes) in enumerate(dataloaders['test']):\n",
        "        inputs = inputs.to(device)\n",
        "        classes = classes.to(device)\n",
        "        outputs = model_tl(inputs)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        for t, p in zip(classes.view(-1), preds.view(-1)):\n",
        "                confusion_matrix[t.long(), p.long()] += 1\n",
        "\n",
        "print(confusion_matrix)"
      ],
      "metadata": {
        "id": "i2j319d93uPq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_confusion_matrix(cm           = np.array([[297.,  39.,  36.,  28.],\n",
        "                                               [  8., 364.,  12.,  16.],\n",
        "                                               [ 14.,   7., 311.,  68.],\n",
        "                                               [ 46.,  25.,  25., 304.]]),\n",
        "                      normalize    = True,\n",
        "                      target_names = ['defect', 'longberry', 'peaberry', 'premium'],\n",
        "                      title        = \"Confusion Matrix\")"
      ],
      "metadata": {
        "id": "kalmlBcE4Zqa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#To get the per-class accuracy:\n",
        "\n",
        "print(confusion_matrix.diag()/confusion_matrix.sum(1))"
      ],
      "metadata": {
        "id": "7xWJ3xwT4xbt"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}